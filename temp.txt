last par of Summary

Use of `compareMCMCs` supports but does not require a primary role for MCMCs created with the `nimble` [@devalpine2017; @nimble] package for hierarchical statistical models.  That is because `nimble` provides greater flexibility than other packages to customize its MCMC system, configuring which samplers will operate on which parts of a model and/or writing new samplers.  Thus, it is of interest to compare multiple MCMC methods all implemented within `nimble`.   Furthermore, `nimble` uses a model language that is a dialect of that used by `WinBUGS`, `OpenBUGS`, `MultiBUGS`, and `JAGS` [@lunn2009bugs; @lunn2010winbugs; @plummer2003jags; @Goudie20].  These packages are often called from R via packages such as `R2WinBUGS` [@R2WinBUGS], `rjags` [@plummer19], and `jagsUI` [@kellner21].  Therefore, for fully compatible models, comparisons between `nimble` and `JAGS` can be run in `compareMCMCs` from the same model and data specifications.  A plug-in is also provided for `Stan` via `rstan` [@Rstan], and the extension system to plug in new MCMC engines is clearly documented.

Statement of need

Many other packages run MCMC algorithms and/or post-process MCMC results, but `compareMCMCs` is distinct in its goal of supporting MCMC research by comparing MCMC methods.  Packages that run MCMC from R are documented on the "Cran Task View" page for "Bayesian Inference" [@park21] of the Comprehensive R Archive Network (CRAN).   Some popular general packages include those listed above as well as others such as `MCMCpack`  [@martin11] and `LaplacesDemon` [@statisticat21].  Furthermore, there are MCMC engines based in Python, such as PyMC [@PyMC], and other languages.  These may be called via appropriate interfaces from R to other languages.

Of the packages listed on the "Bayesian Inference" Task View, only the `SamplerCompare` [@samplerCompare] package appears to specifically support the goal of comparing MCMC performance.  However, this package can only compare MCMC samplers that have exactly one scalar tuning parameter, target distributions that are continuous with constant dimension, and are implemented within the package.

Packages for post-processing of MCMC samples (e.g., `coda` [@coda], `BayesPostEst` [@BayesPostEst], and `MCMCvis` [@MCMCvis]) aim to provide features for scientific summary and presentation of results, whereas `compareMCMCs` provides features for comparisons of algorithm performance across packages.  Assessing MCMC performance is not simply a matter of computational benchmarking.  For example, effective sample size is itself a non-trivial property to estimate by statistical methods, different metrics may be of interest for different purposes, and consistency of algorithm results between different MCMC engines can only be determined statistically, i.e. within simulation error.   Therefore, the features needed for comparing MCMC performance are distinct from those needed for presenting scientific results based on MCMC.
